\documentclass[11pt,leqno]{article}
\usepackage[spanish,activeacute]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{listings}
\usepackage{amsthm}
\usepackage[hidelinks]{hyperref}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{pgfplotstable}
\usepackage{verbatim}
\usepackage{geometry}
\usepackage{changepage}

\author{Jacinto Carrasco Castillo 	\\
		N.I.F. 32056356-Z			\\ 
		\href{jacintocc@correo.ugr.es}{jacintocc@correo.ugr.es}}
		
\title{	Práctica 1 Metaheurísticas.\\
		Búsqueda por trayectorias para el problema \\
		de la selección de características\\
		Curso 15-16\\
		Algoritmos: SFS, LS, SA, TS, Extended TS}

\newcommand{\maketable}[1]{
\begin{adjustwidth}{-1cm}{}
\resizebox{\linewidth}{!}{
\pgfplotstabletypeset[
	every head row/.style={
		before row={%
				\hline
				& \multicolumn{4}{c|}{WDBC} & \multicolumn{4}{c|}{Movement Libras} & \multicolumn{4}{c|}{Arrythmia}\\
				\cline{2-13}
		},
		after row=\cline{2-13}\hline,
		column type=c
	},
	every first column/.style={ column type/.add={|}{} },
	every last row/.style={before row =\hline\hline, after row=\hline},
	column type/.add={}{|},
	columns/partition/.style={column name = , string type},
	columns/in/.style={column name =\%Clas. in},
	columns/inL/.style={column name =\%Clas. in},	
	columns/inA/.style={column name =\%Clas. in},
	columns/out/.style={column name =\%Clas. out},
	columns/outL/.style={column name =\%Clas. out},	
	columns/outA/.style={column name =\%Clas. out},
	columns/T/.style={column name =T},
	columns/tA/.style={column name =T},
	columns/tL/.style={column name =T},
	columns/red./.style={column name =\%red.},
	columns/redL/.style={column name =\%red.},	
	columns/redA/.style={column name =\%red.},
	precision=4
	]{#1}
}
\end{adjustwidth}
}

\newcommand{\makeresume}[4]{
\pgfplotstablevertcat{#1}{#2}
\pgfplotstablecreatecol[copy column from table={#3}{in}]{inL} {#1}
\pgfplotstablecreatecol[copy column from table={#3}{out}]{outL} {#1}
\pgfplotstablecreatecol[copy column from table={#3}{red}]{redL} {#1}
\pgfplotstablecreatecol[copy column from table={#3}{T}]{tL} {#1}
\pgfplotstablecreatecol[copy column from table={#4}{in}]{inA} {#1}
\pgfplotstablecreatecol[copy column from table={#4}{out}]{outA} {#1}
\pgfplotstablecreatecol[copy column from table={#4}{red}]{redA} {#1}
\pgfplotstablecreatecol[copy column from table={#4}{T}]{tA} {#1}
}

\newcommand{\getElement}[3]{\pgfplotstablegetelem{#2}{#3}\of{#1} \pgfplotsretval}


\begin{document}

% Tablas KNN
\pgfplotstableread[col sep=comma]{Resultados/wKNN.csv}\datawKNN
\pgfplotstableread[col sep=comma]{Resultados/lKNN.csv}\datalKNN
\pgfplotstableread[col sep=comma]{Resultados/aKNN.csv}\dataaKNN
   
% Tablas Greedy SFS
\pgfplotstableread[col sep=comma]{Resultados/wSFS.csv}\datawSFS
\pgfplotstableread[col sep=comma]{Resultados/lSFS.csv}\datalSFS
\pgfplotstableread[col sep=comma]{Resultados/aSFS.csv}\dataaSFS

% Tablas Local Search
\pgfplotstableread[col sep=comma]{Resultados/wLS.csv}\datawLS
\pgfplotstableread[col sep=comma]{Resultados/lLS.csv}\datalLS
\pgfplotstableread[col sep=comma]{Resultados/aLS.csv}\dataaLS

% Tablas Simulated Annealing
\pgfplotstableread[col sep=comma]{Resultados/wSA.csv}\datawSA
\pgfplotstableread[col sep=comma]{Resultados/lSA.csv}\datalSA
\pgfplotstableread[col sep=comma]{Resultados/aSA.csv}\dataaSA

% Tablas Tabu Search
\pgfplotstableread[col sep=comma]{Resultados/wTS.csv}\datawTS
\pgfplotstableread[col sep=comma]{Resultados/lTS.csv}\datalTS
\pgfplotstableread[col sep=comma]{Resultados/aTS.csv}\dataaTS

% Tablas Extended Tabu Search
\pgfplotstableread[col sep=comma]{Resultados/wETS.csv}\datawETS
\pgfplotstableread[col sep=comma]{Resultados/lETS.csv}\datalETS
\pgfplotstableread[col sep=comma]{Resultados/aETS.csv}\dataaETS

\begin{titlepage}
\maketitle
\end{titlepage}

\tableofcontents
\newpage

\section{Descripción del problema}

El problema que nos ocupa es un problema de clasificación. Partimos de una muestra de los objetos que queremos clasificar y su clasificación, es decir, la clase a la que pertenece y pretendemos, en base a esta muestra, poder clasificar nuevas instancias que nos lleguen.\\
La clasificación se realizará en base a una serie de características, que nos permitan determinar si un individuo pertenece a un grupo u otro. Por tanto, tendremos individuos de una población $\Omega$ representados como un vector de características: $ \omega \in \Omega; \omega = (x_1(\omega), \dots x_n(\omega))$, donde $\omega$ es un individuo de la población y $x_i, i=1,\dots n$ son las $n$ características sobre las que se tiene información. Buscamos $f:\Omega \longrightarrow C=\{C_1, \dots, C_M\}$, donde $C=\{C_1, \dots, C_M\}$ es el conjunto de clases a las que podemos asignar los objetos.\\
El problema de clasificación está relacionado con la separabilidad de las clases en el sentido de que existirá la función $f$  anteriormente mencionada siempre que las clases sean separables, es decir, siempre que un individuo con unas mismas características pertenzcan a una misma clase. Sin embargo, si se da que dos individuos $\omega_1, \omega_2 \in \Omega$, $(x_1(\omega_1), \dots x_n(\omega_1))=(x_1(\omega_2), \dots x_n(\omega_2))$ y sin embargo $f(\omega_1) \neq f(\omega_2)$, no podrá existir $f$. En todo caso, querríamos obtener la mayor tasa de acierto posible.\\  
Por tanto, queremos, en base a unos datos, hallar la mejor $f$ posible. De esto trata el aprendizaje clasificado: Se conocen instancias de los datos y las clases a las que pertenecen. Usaremos como técnica de aprendizaje supervisado la técnica estadística conocida como $k$ vecinos más cercanos. Se trata de buscar los $k$ vecinos más cercanos y asignar al objeto la clase que predomine de entre los vecinos. En caso de empate, se seleccionará la clase con más votos más cercana.\\  
Pero no nos quedamos en el problema de clasificación, sino que buscamos reducir el número de características. Con esto pretendemos seleccionar las características que nos den un mejor resultado (por ser las más influyentes a la hora de decidir la categoría). Usaremos los datos de entrenamiento haciendo pruebas mediante diferentes metaheurísticas hasta obtener la mejor selección que seamos capaces de encontrar.\\  
El interés en realizar la selección de características reside en que se aumentará la eficiencia, al requerir menos tiempo para construir el clasificador, y que se mejoran los resultados al descartar las características menos influyentes y que sólo aportan ruido. Esto hace también que se reduzcan los costes de mantenimiento y se aumente la interpretabilidad de los datos.\\
Las funciones de evaluación pueden estar basadas en la consistencia, en la Teoría de la Información, en la distancia o en el rendimiento de clasificadores. Nosotros usaremos el rendimiento promedio de un clasificador $3-NN$.


\section{Descripción de la aplicación de los algoritmos}
\subsection{Representación de soluciones}
	Para este problema tenemos varias formas posibles de representar las soluciones:
	\begin{itemize}
		\item Representación binaria: Cada solución está representada por un vector binario de longitud igual al número de características, donde las posiciones seleccionadas tendrán un 1 o $\texttt{True}$ y las no seleccionadas un 0 o $\texttt{False}$. Esta opción, que será la que tomaremos, sólo es recomendable si no tenemos restricciones sobre el número de características seleccionadas.
		\item Representación entera: Cada solución es un vector de tamaño fijo $m \leq n$ con las características seleccionadas. Esta representación sí es adecuada si tenemos restricciones sobre el número de características tomadas ya que no podemos hacerlo con más de $m$.
		\item Representación de orden: Cada solución es una permutación de $n$ elementos, ordenados según la importancia de cada característica. Aquí también se maneja el cumplimiento de restricciones pues una vez encontrada la solución, tomaremos sólo las primeras $m$ características.
	\end{itemize}
	Se ha de mencionar que en las dos últimas representaciones el espacio de soluciones es mayor que el espacio de búsqueda, justificado en la representación de orden porque da más información (podríamos tomar soluciones de longitud variable), pero que en la representación entera sólo es razonable asumir si tenemos una restricción de longitud fija. Además, otra ventaja de la representación binaria es la facilidad para aplicarle operadores (de vecindario, en posteriores prácticas de cruce...) manteniendo la consistencia.

\subsection{Función objetivo}
	La función objetivo será el porcentaje de acierto en el conjunto de test para el clasificador $3-NN$ obtenido usando las distancias de los individuos $\omega$ en las dimensiones representadas por las características seleccionadas en el vector solución para el conjunto de entrenamiento. El objetivo será maximizar esta función. A la hora de buscar esta solución sólo estaremos manejando los datos de entrenamiento, luego aquí la función objetivo será la media de tasa de acierto para cada uno de los datos de entrenamiento con respecto a todos los demás, por lo que tenemos que usar la técnica de $\textit{Leave-One-Out}$. Esta técnica consiste en quitar del conjunto de datos cada uno de los elementos, comprobar el acierto o no para este dato en concreto, y devolverlo al conjunto de datos. Así evitamos que los resultados estén sesgados.
	
\begin{lstlisting}[mathescape=true]
targetFunction(data_train, categories_train, data_test,
				categories_test, solution):
BEGIN
	num_items $\leftarrow$ length(data_test)
	sum_score $\leftarrow$ 0
	
	data_train' $\leftarrow$ {$col_i$ from data_train if $solution_i$ is True}
	classifier $\leftarrow$ Make3NNClasifier(data_train', categories_train)

	data_test' $\leftarrow$ {$col_i$ from data_test if $solution_i$ is True}
	
	FOR item IN data_test'
		predicted_class $\leftarrow$ classifier(item)
		IF predicted_class == categories_test_item THEN
			sum_score $\leftarrow$ sum_score + 1
	END
	
	RETURN sum_score / num_items * 100 
END
\end{lstlisting}

	La función de evaluación incluida en el código no la he realizado yo sino que he utilizado el paquete $\texttt{sklearn}$ de $\texttt{Python}$ por razones de eficiencia.
	
\subsection{Operadores comunes}
	Entenderemos como vecindario de una solución a los vectores que sólo difieren en una posición. Por tanto, el operador para movernos a una solución vecina consistirá en cambiar una posición determinada:
\begin{lstlisting}[mathescape=true]
flip(solution, positon):
BEGIN
	neighbour $\leftarrow$ copy(solution)
	actual_value $\leftarrow$ $\texttt{solution}_{\texttt{position}}$
	$\texttt{neighbour}_{\texttt{position}}$ $\leftarrow$ NOT actual_value
	RETURN neighbour
END
\end{lstlisting}
	

\section{Estructura del método de búsqueda}
\subsection{Búsqueda local}
\subsection{Enfriamiento simulado}
\subsection{Búsqueda tabú}
\subsection{Búsqueda tabú extendida}

\section{Algoritmo de comparación}

\section{Procedimiento para desarrollar la práctica}

\section{Experimentos y análisis de resultados}

% Tablas Resumen KNN
\makeresume{\dataKNN}{\datawKNN}{\datalKNN}{\dataaKNN}

% Salida por pantalla
\maketable{\dataKNN}

% Tablas Resumen SFS
\makeresume{\dataSFS}{\datawSFS}{\datalSFS}{\dataaSFS}

% Salida por pantalla
\maketable{\dataSFS}

% Tablas Resumen LS
\makeresume{\dataLS}{\datawLS}{\datalLS}{\dataaLS}

% Salida por pantalla
\maketable{\dataLS}

% Tablas Resumen SA
\makeresume{\dataSA}{\datawSA}{\datalSA}{\dataaSA}

% Salida por pantalla
\maketable{\dataSA}

% Tablas Resumen TS
\makeresume{\dataTS}{\datawTS}{\datalTS}{\dataaTS}

% Salida por pantalla
\maketable{\dataTS}

% Tablas Resumen ETS
\makeresume{\dataETS}{\datawETS}{\datalETS}{\dataaETS}

% Salida por pantalla
\maketable{\dataETS}

% Tabla Global
\pgfplotstableread[col sep=comma]{
algorithm,inW,outW,redW,TW,inL,outL,redL,TL,inA,outA,redA,TA
3-NN,	\getElement{\dataKNN}{10}{in},\getElement{\dataKNN}{10}{out},\getElement{\dataKNN}{10}{red},\getElement{\dataKNN}{10}{T},\getElement{\dataKNN}{10}{inL},\getElement{\dataKNN}{10}{outL},\getElement{\dataKNN}{10}{redL},\getElement{\dataKNN}{10}{tL},\getElement{\dataKNN}{10}{inA},\getElement{\dataKNN}{10}{outA},\getElement{\dataKNN}{10}{redA},\getElement{\dataKNN}{10}{tA}
SFS,	\getElement{\dataSFS}{10}{in},\getElement{\dataSFS}{10}{out},\getElement{\dataSFS}{10}{red},\getElement{\dataSFS}{10}{T},\getElement{\dataSFS}{10}{inL},\getElement{\dataSFS}{10}{outL},\getElement{\dataSFS}{10}{redL},\getElement{\dataSFS}{10}{tL},\getElement{\dataSFS}{10}{inA},\getElement{\dataSFS}{10}{outA},\getElement{\dataSFS}{10}{redA},\getElement{\dataSFS}{10}{tA}
BL,		\getElement{\dataLS}{10}{in},\getElement{\dataLS}{10}{out},\getElement{\dataLS}{10}{red},\getElement{\dataLS}{10}{T},\getElement{\dataLS}{10}{inL},\getElement{\dataLS}{10}{outL},\getElement{\dataLS}{10}{redL},\getElement{\dataLS}{10}{tL},\getElement{\dataLS}{10}{inA},\getElement{\dataLS}{10}{outA},\getElement{\dataLS}{10}{redA},\getElement{\dataLS}{10}{tA}
ES,		\getElement{\dataSA}{10}{in},\getElement{\dataSA}{10}{out},\getElement{\dataSA}{10}{red},\getElement{\dataSA}{10}{T},\getElement{\dataSA}{10}{inL},\getElement{\dataSA}{10}{outL},\getElement{\dataSA}{10}{redL},\getElement{\dataSA}{10}{tL},\getElement{\dataSA}{10}{inA},\getElement{\dataSA}{10}{outA},\getElement{\dataSA}{10}{redA},\getElement{\dataSA}{10}{tA}
BT básica,	\getElement{\dataTS}{10}{in},\getElement{\dataTS}{10}{out},\getElement{\dataTS}{10}{red},\getElement{\dataTS}{10}{T},\getElement{\dataTS}{10}{inL},\getElement{\dataTS}{10}{outL},\getElement{\dataTS}{10}{redL},\getElement{\dataTS}{10}{tL},\getElement{\dataTS}{10}{inA},\getElement{\dataTS}{10}{outA},\getElement{\dataTS}{10}{redA},\getElement{\dataTS}{10}{tA}
BT extendida,	\getElement{\dataETS}{10}{in},\getElement{\dataETS}{10}{out},\getElement{\dataETS}{10}{red},\getElement{\dataETS}{10}{T},\getElement{\dataETS}{10}{inL},\getElement{\dataETS}{10}{outL},\getElement{\dataETS}{10}{redL},\getElement{\dataETS}{10}{tL},\getElement{\dataETS}{10}{inA},\getElement{\dataETS}{10}{outA},\getElement{\dataETS}{10}{redA},\getElement{\dataETS}{10}{tA}
}\dataGlobal

\begin{adjustwidth}{-1cm}{}
\resizebox{\linewidth}{!}{
\pgfplotstabletypeset[
	every head row/.style={
		before row={%
				\hline
				& \multicolumn{4}{c|}{WDBC} & \multicolumn{4}{c|}{Movement Libras} & \multicolumn{4}{c|}{Arrythmia}\\
				\cline{2-13}
		},
		after row=\cline{2-13}\hline,
		column type=c
	},
	every first column/.style={ column type/.add={|}{} },
	every last row/.style={after row=\hline},
	column type/.add={}{|},
	columns/algorithm/.style={column name = , string type},
	columns/inW/.style={sci,  precision=4,column name =\%Clas. in},
	columns/inL/.style={column name =\%Clas. in},	
	columns/inA/.style={column name =\%Clas. in},
	columns/outW/.style={column name =\%Clas. out},
	columns/outL/.style={column name =\%Clas. out},	
	columns/outA/.style={column name =\%Clas. out},
	columns/TW/.style={column name =T},
	columns/TA/.style={column name =T},
	columns/TL/.style={column name =T},
	columns/redW/.style={column name =\%red.},
	columns/redL/.style={column name =\%red.},	
	columns/redA/.style={column name =\%red.},
	string type
	]{\dataGlobal}
}
\end{adjustwidth}

\section{Bibliografía}
\begin{itemize}
\item \href{https://www.complang.tuwien.ac.at/doc/texlive-pictures-doc/latex/pgfplots/pgfplotstable.pdf}{Manual PGFPLOTSTABLE}
\end{itemize}
\end{document}
