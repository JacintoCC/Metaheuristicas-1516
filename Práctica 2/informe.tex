\documentclass[11pt,leqno]{article}
\usepackage[spanish,activeacute]{babel}
\usepackage[utf8]{inputenc}
\usepackage{etex}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{listings}
\usepackage{amsthm}
\usepackage[hidelinks]{hyperref}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.12}
\usepackage{verbatim}
\usepackage{geometry}
\usepackage{changepage}
\usepackage{pgfplots}
\usepackage{float}
\usepgfplotslibrary{colorbrewer}

% Título y autor
\author{Jacinto Carrasco Castillo 	\\
		N.I.F. 32056356-Z			\\ 
		\href{jacintocc@correo.ugr.es}{jacintocc@correo.ugr.es}}
		
\title{	Práctica 2 Metaheurísticas.\\
		Búsquedas multiarranque para el problema \\
		de la selección de características}

% Comando para mostrar una tabla de resultados.
%    - El argumento se corresponde con la tabla
\newcommand{\maketable}[1]{
\begin{adjustwidth}{-1cm}{}
\resizebox{\linewidth}{!}{
\pgfplotstabletypeset[
	every head row/.style={
		before row={%
				\hline
				& \multicolumn{4}{c|}{WDBC} & \multicolumn{4}{c|}{Movement Libras} & \multicolumn{4}{c|}{Arrhythmia}\\
				\cline{2-13}
		},
		after row=\cline{2-13}\hline,
		column type=c
	},
	every first column/.style={ column type/.add={|}{} },
	every last row/.style={ after row=\hline},
	every row no 10/.style={before row=\hline\hline},
	column type/.add={}{|},
	columns/partition/.style={column name = , string type},
	columns/in/.style={column name =\%Clas. in},
	columns/inL/.style={column name =\%Clas. in},	
	columns/inA/.style={column name =\%Clas. in},
	columns/out/.style={column name =\%Clas. out},
	columns/outL/.style={column name =\%Clas. out},	
	columns/outA/.style={column name =\%Clas. out},
	columns/T/.style={column name =T},
	columns/tA/.style={column name =T},
	columns/tL/.style={column name =T},
	columns/red./.style={column name =\%red.},
	columns/redL/.style={column name =\%red.},	
	columns/redA/.style={column name =\%red.},
	precision=4
	]{#1}
}
\end{adjustwidth}
}

% Comando para formar una tabla que reúna en una tabla
% los resultados de las tres BD.
% Argumentos:
%  - Tabla formada
%  - Tabla WDBC
%  - Tabla Libras
%  - Tabla Arrhythmia
\newcommand{\makeresume}[4]{
\pgfplotstablevertcat{#1}{#2}
\pgfplotstablecreatecol[copy column from table={#3}{in}]{inL} {#1}
\pgfplotstablecreatecol[copy column from table={#3}{out}]{outL} {#1}
\pgfplotstablecreatecol[copy column from table={#3}{red}]{redL} {#1}
\pgfplotstablecreatecol[copy column from table={#3}{T}]{tL} {#1}
\pgfplotstablecreatecol[copy column from table={#4}{in}]{inA} {#1}
\pgfplotstablecreatecol[copy column from table={#4}{out}]{outA} {#1}
\pgfplotstablecreatecol[copy column from table={#4}{red}]{redA} {#1}
\pgfplotstablecreatecol[copy column from table={#4}{T}]{tA} {#1}
}

% Método para obtener un elemento de una tabla
% Argumentos:
%  - Tabla
%  - Fila
%  - Columna
\newcommand{\getElement}[3]{\pgfplotstablegetelem{#2}{#3}\of{#1} \pgfplotsretval}


\begin{document}

% Tablas KNN
\pgfplotstableread[col sep=comma]{Software/Resultados/wKNN.csv}\datawKNN
\pgfplotstableread[col sep=comma]{Software/Resultados/lKNN.csv}\datalKNN
\pgfplotstableread[col sep=comma]{Software/Resultados/aKNN.csv}\dataaKNN
   
% Tablas Greedy SFS
\pgfplotstableread[col sep=comma]{Software/Resultados/wSFS.csv}\datawSFS
\pgfplotstableread[col sep=comma]{Software/Resultados/lSFS.csv}\datalSFS
\pgfplotstableread[col sep=comma]{Software/Resultados/aSFS.csv}\dataaSFS

% Tablas BMB
\pgfplotstableread[col sep=comma]{Software/Resultados/wBMB.csv}\datawBMB
\pgfplotstableread[col sep=comma]{Software/Resultados/lBMB.csv}\datalBMB
\pgfplotstableread[col sep=comma]{Software/Resultados/aBMB.csv}\dataaBMB

% Tablas GRASP
\pgfplotstableread[col sep=comma]{Software/Resultados/wGRASP.csv}\datawGRASP
\pgfplotstableread[col sep=comma]{Software/Resultados/lGRASP.csv}\datalGRASP
\pgfplotstableread[col sep=comma]{Software/Resultados/aGRASP.csv}\dataaGRASP

% Tablas ILS
\pgfplotstableread[col sep=comma]{Software/Resultados/wILS.csv}\datawILS
\pgfplotstableread[col sep=comma]{Software/Resultados/lILS.csv}\datalILS
\pgfplotstableread[col sep=comma]{Software/Resultados/aILS.csv}\dataaILS

% Tabla Resumen
\pgfplotstableread[col sep=comma]{Software/Resultados/results.csv}\dataGlobal


% Portada y título
\maketitle

\begin{center}
Curso 2015-2016\\

Problema de Selección de Características.\\ 

Grupo de prácticas: Viernes 17:30-19:30\\

Quinto curso del Doble Grado en Ingeniería Informática y Matemáticas.\\
\textit{ }\\
\end{center}

Algoritmos considerados:
\begin{enumerate}
\item Búsqueda Multiarranque Básica
\item GRASP
\item ILS
\end{enumerate}

\newpage


% Índice
\tableofcontents
\newpage

\section{Descripción del problema}

El problema que nos ocupa es un problema de clasificación. Partimos de una muestra de los objetos que queremos clasificar y su etiqueta, es decir, la clase a la que pertenece y pretendemos, en base a esta muestra, poder clasificar nuevas instancias que nos lleguen.\\
La clasificación se realizará en base a una serie de características, que nos permitan determinar si un individuo pertenece a un grupo u otro. Por tanto, tendremos individuos de una población $\Omega$ representados como un vector de características: $ \omega \in \Omega; \omega = (x_1(\omega), \dots x_n(\omega))$, donde $\omega$ es un individuo de la población y $x_i, i=1,\dots n$ son las $n$ características sobre las que se tiene información. Buscamos $f:\Omega \longrightarrow C=\{C_1, \dots, C_M\}$, donde $C=\{C_1, \dots, C_M\}$ es el conjunto de clases a las que podemos asignar los objetos.\\
El problema de clasificación está relacionado con la separabilidad de las clases en el sentido de que existirá la función $f$  anteriormente mencionada siempre que las clases sean separables, es decir, siempre que un individuo con unas mismas características pertenzcan a una misma clase. Sin embargo, si se diese que dos individuos $\omega_1, \omega_2 \in \Omega$, $(x_1(\omega_1), \dots, x_n(\omega_1))=(x_1(\omega_2), \dots, x_n(\omega_2))$ y sin embargo $f(\omega_1) \neq f(\omega_2)$, no podrá existir $f$. En todo caso, querríamos obtener la mayor tasa de acierto posible.\\  
Por tanto, tratamos, en base a unos datos, hallar la mejor $f$ posible. De esto trata el aprendizaje supervisado: Se conocen instancias de los datos y las clases a las que pertenecen. Usaremos como técnica de aprendizaje supervisado la técnica estadística conocida como $k$ vecinos más cercanos. Se trata de buscar los $k$ vecinos más cercanos y asignar al objeto la clase que predomine de entre los vecinos. En caso de empate, se seleccionará la clase con más votos más cercana.\\  
Pero no nos quedamos en el problema de clasificación, sino que buscamos reducir el número de características. Con esto pretendemos seleccionar las características que nos den un mejor resultado (por ser las más influyentes a la hora de decidir la categoría). Usaremos los datos de entrenamiento haciendo pruebas mediante diferentes metaheurísticas hasta obtener la mejor selección que seamos capaces de encontrar.\\  
El interés en realizar la selección de características reside en que se aumentará la eficiencia, al requerir menos tiempo para construir el clasificador, y que se mejoran los resultados al descartar las características menos influyentes y que sólo aportan ruido. Esto hace también que se reduzcan los costes de mantenimiento y se aumente la interpretabilidad de los datos.\\
Las funciones de evaluación pueden estar basadas en la consistencia, en la Teoría de la Información, en la distancia o en el rendimiento de clasificadores. Nosotros usaremos el rendimiento promedio de un clasificador $3-NN$.


\section{Descripción de la aplicación de los algoritmos}
\subsection{Representación de soluciones}
	Para este problema tenemos varias formas posibles de representar las soluciones:
	\begin{itemize}
		\item Representación binaria: Cada solución está representada por un vector binario de longitud igual al número de características, donde las posiciones seleccionadas tendrán un 1 o $\texttt{True}$ y las no seleccionadas un 0 o $\texttt{False}$. Esta opción, que será la que tomaremos, sólo es recomendable si no tenemos restricciones sobre el número de características seleccionadas.
		\item Representación entera: Cada solución es un vector de tamaño fijo $m \leq n$ con las características seleccionadas. Esta representación sí es adecuada si tenemos restricciones sobre el número de características tomadas ya que no podemos hacerlo con más de $m$ características.
		\item Representación de orden: Cada solución es una permutación de $n$ elementos, ordenados según la importancia de cada característica. Aquí también se maneja el cumplimiento de restricciones pues una vez encontrada la solución, tomaremos sólo las primeras $m$ características.
	\end{itemize}
	Se ha de mencionar que en las dos últimas representaciones el espacio de soluciones es mayor que el espacio de búsqueda, justificado en la representación de orden porque da más información (podríamos tomar soluciones de longitud variable), pero que en la representación entera sólo es razonable asumir si tenemos una restricción de longitud fija. Además, otra ventaja de la representación binaria es la facilidad para aplicarle operadores (de vecindario, en posteriores prácticas de cruce...) manteniendo la consistencia.

\subsection{Función objetivo}
	La función objetivo será el porcentaje de acierto en el conjunto de test para el clasificador $3-NN$ obtenido usando las distancias de los individuos $\omega$ en las dimensiones representadas por las características seleccionadas en el vector solución para el conjunto de entrenamiento. El objetivo será maximizar esta función. A la hora de buscar esta solución sólo estaremos manejando los datos de entrenamiento, luego aquí la función objetivo será la media de tasa de acierto para cada uno de los datos de entrenamiento con respecto a todos los demás, por lo que tenemos que usar la técnica de $\textit{Leave-One-Out}$. Esta técnica consiste en quitar del conjunto de datos cada uno de los elementos, comprobar el acierto o no para este dato en concreto, y devolverlo al conjunto de datos. Así evitamos que los resultados estén sesgados a favor de la clase o etiqueta original, al contar siempre con un voto la clase verdadera.\\
	La implementación de la función objetivo (obtener el score para Test) la he realizado basándome en el código paralelizado realizado para CUDA por Alejandro García Montoro para la función de $\textit{Leave One Out}$. El pseudocódigo incluido se trata del esquema seguido para cada proceso, esto es, cada elemento en el conjunto de datos de entrenamiento, puesto que el método en $\texttt{Python}$ para pasarle a la GPU los datos de entrenamiento, test, categorías y un puntero con la solución no tiene mayor interés.
	
\begin{lstlisting}[mathescape=true]
targetFunction(data_train, categories_train, 
               data_test,  categories_test):
BEGIN
   test $\leftarrow$ Num. Process
   num_test $\leftarrow$ length(data_test)
   
   $\textbf{\texttt{exit}}$ IF test > num_test
   my_features $\leftarrow$ data_test[test]
   
   k_nearest $\leftarrow$ initialize3Neighbours
	
   FOR item IN data_train
      distance $\leftarrow$ computeDistance(my_features, item)
      k_nearest $\leftarrow$ update(item, distance)
   END
	
   class $\leftarrow$ poll(classes of k_nearest)
      
   RETURN class == categories_test[test]
END
\end{lstlisting}

	Esto en \texttt{CUDA} lo que hace es guardarnos, para cada proceso, si se ha acertado o no. Posteriormente se pasa el vector con cada resultado (cada ejecución de este código) de nuevo a \texttt{Python} y se calcula el porcentaje de aciertos. Nótese que no se realiza la proyección por las características seleccionadas, esto lo hacemos al pasar los datos.\\
	Para la función de evaluación de las posibles soluciones que se van generando durante la búsqueda utilizo el método realizado por Alejandro García Montoro para usar CUDA. El algoritmo es similar al anterior, pero incluye \textit{Leave One Out}:

\begin{lstlisting}[mathescape=true]
targetFunctionLeaveOneOut(data_train, categories_train):
BEGIN
   sample $\leftarrow$ Num. Process
   num_samples $\leftarrow$ length(data_train)
   
   $\textbf{\texttt{exit}}$ if sample > num_samples
   my_features $\leftarrow$ data_train[sample]
   
   k_nearest $\leftarrow$ initialize3Neighbours
	
   FOR item IN data_train
    IF item $\neq$ sample THEN
      distance $\leftarrow$ computeDistance(my_features, item)
      k_nearest $\leftarrow$ update(item, distance)
   END
	
   class $\leftarrow$ poll(classes of k_nearest)
      
   RETURN class == categories_train[sample]
END
\end{lstlisting}

\subsection{Operadores comunes}
	Entenderemos como vecindario de una solución a los vectores que sólo difieren en una posición. Por tanto, el operador para movernos a una solución vecina consistirá en cambiar una posición determinada:
\begin{lstlisting}[mathescape=true]
flip(solution, positon):
BEGIN
	neighbour $\leftarrow$ copy(solution)
	actual_value $\leftarrow$ $\texttt{solution}_{\texttt{position}}$
	$\texttt{neighbour}_{\texttt{position}}$ $\leftarrow$ NOT actual_value
	RETURN neighbour
END
\end{lstlisting}
	
	
En esta práctica también es común a todos los métodos el método de búsqueda local. En primer lugar, para favorecer la reducción de características, probé a considerar como mejor solución aquella que con la misma calidad en términos de porcentaje de aciertos, tuviera un menor número de características seleccionadas. Sin embargo, esto hacía incrementar notablemente el tiempo de búsqueda, especialmente en la base de datos \texttt{Arrhythmia}, pues al haber tantas características ocupaba gran parte del tiempo de la búsqueda local en reducir el número de seleccionadas en lugar de intentar acceder a posiciones mejores. Sí consideraremos que éstas son mejores soluciones, o mejor dicho, comprobaremos que son mejores soluciones fuera de la búsqueda local.\\
Como diferencia con respecto a la primera práctica, ahora la solución inicial es un parámetro y no se genera aleatoriamente dentro del método, ya que depende del algoritmo de búsqueda por trayectorias múltiples que usemos:

	\begin{lstlisting}[mathescape=true]
localSearch(data, categories, solution) BEGIN	
   REPEAT
      first_neig $\leftarrow$  random{1,..., num_features}
      neighbours $\leftarrow$ {flip(solution,i): 
                            i=first_neig,...,first_neig-1}
      found_better $\leftarrow$ FALSE
		
      FOR neig IN neighbours
         IF neigh is better than solution THEN
            found_better $\leftarrow$ TRUE
            solution $\leftarrow$ neig
            BREAK
		
         END
   WHILE( found_better )
	
   return solution
END
	\end{lstlisting}
	
	Donde que el vecino sea mejor que la solución actual significa que tiene una mayor tasa de acierto en el conjunto de datos de entrenamiento.

\section{Estructura del método de búsqueda}

\subsection{Búsqueda multiarranque básica}

Este método consiste en la realización de diferentes búsquedas locales que parten de soluciones aleatorias. Para ello, simplemente se lanzan, como se indica en el guión de prácticas, 25 búsquedas locales y devolvemos la mejor solución encontrada. El criterio para considerar que una solución es mejor que otra se basa, como se ha dicho previamente, en encontrar el vector de características que ofrezca una mejor tasa de acierto y, de entre ellas, la que tenga menos características seleccionadas. Los resultados esperados serán similares a las mejores ejecuciones del apartado de búsqueda local en la práctica anterior. Se podrá observar también si el hecho de incluir aceptar como mejores soluciones con igual tasa de acierto y menor número de características beneficia o perjudica a la tasa de acierto fuera de los datos de entrenamiento.

	\newpage \begin{lstlisting}[mathescape=true]
BMB(data, categories) BEGIN	
   num_features $\leftarrow$ length ( data[0] )
   best_solution $\leftarrow$ {F,...,F: size = num_features}
   
   FOR i = 1,...,25
      solution $\leftarrow$ random({True,False}, size = num_features)
                                   
      solution $\leftarrow$ localSearch(data, categories, solution)
      IF solution is better than best_solution THEN
         best_solution $\leftarrow$ solution
   END
	
   return best_solution
END
	\end{lstlisting}

\subsection{GRASP}

	El esquema general del algoritmo $\texttt{GRASP}$ es similar al de $\texttt{BMB}$, con la única salvedad de que la solución inicial para cada una de las iteraciones no es un vector aleatorio sino que se trata de una solución aportada por un algoritmo \textit{greedy}. Esto repercute en que es necesario un tiempo mayor de ejecución al ser más costoso lógicamente ejecutar el algoritmo \textit{greedy} que obtener una solución aleatoria. Sin embargo, este tiempo no se pierde en vano, pues resulta lógico pensar \textit{a priori} que si partimos de una buena solución y entonces aplicamos búsqueda local, se llegará a una solución mejor que partiendo de una aleatoria.
	
		\begin{lstlisting}[mathescape=true]
GRASP(data, categories) BEGIN	
   num_features $\leftarrow$ length ( data[0] )
   best_solution $\leftarrow$ {F,...,F: size = num_features}
   
   FOR i = 1,...,25
      solution $\leftarrow$ getGreedySolution(data, categories)
                                   
      solution $\leftarrow$ localSearch(data, categories, solution)
      IF solution is better than best_solution THEN
         best_solution $\leftarrow$ solution
   END
	
   return best_solution
END
	\end{lstlisting}
	
	El interés de este método reside, pues, en la obtención de la solución \textit{greedy}. Aquí también hay diferencias con respecto a la primera práctica. Ahora no se trata de ir seleccionando la mejor característica cada vez, sino de ir seleccionando una característica al azar de las que sobrepasen un umbral. Esto añade mayor variabilidad al espacio de búsqueda explorado, con lo que tenemos mayor probabilidad de encontrar una mejor solución. 
	
	
	\begin{lstlisting}[mathescape=true]
getGreedySolution(data, categories) BEGIN
   solution $\leftarrow$ {F,...,F: size = num_features}
   tolerance $\leftarrow$ 0.3
   
   REPEAT
      char $\leftarrow$ getCharWithThreshold(data,categories,
                                   solution,tolerance)
      
      IF flip(solution, char) is better than solution THEN
         solution $\leftarrow$ flip(solution, char)
   
   WHILE flip(solution, char) is better than solution
   
   RETURN solution
END
	\end{lstlisting}
	
	Aún falta por comentar cómo se obtiene la característica con la que seleccionaremos una característica al azar de entre las que cumplan una cierta condición, que consiste en tener un porcentaje de acierto entre el máximo valor del vecindario y éste menos un $30\%$ de la diferencia entre él y el menor valor del vecindario. El porcentaje de la diferencia aceptada es lógicamente variable, con un valor cercano a $0\%$ se trataría del \textit{greedy} \texttt{SFS}, mientras que con un valor del $100\%$ se trata de una selección de un vecino aleatorio, y parando el algoritmo cuando seleccionásemos a un vecino peor que la solución actual.
	
	\begin{lstlisting}[mathescape=true]
getCharWithThreshold(data, categories,solution,tolerance) BEGIN
   values $\leftarrow$ vector( size = num_features )
   FOR i = 1,..., num_features
      
      neig $\leftarrow$ neighbours[i]
      values[i] $\leftarrow$ score(data, categories, neig)
   
   END
   
   max $\leftarrow$ max(values)
   min $\leftarrow$ min(values)   
   above_threshold $\leftarrow$ {i $\in$ {1,...,N: 
                         values[i] > max - tolerance(max-min)}
   char $\leftarrow$ random(above_threshold)
   
   RETURN char 
END
	\end{lstlisting}
	
	
	
\subsection{ILS}

	Para la búsqueda local iterada, se trata también de realizar 25 iteraciones al igual que en los métodos anteriores. La diferencia ahora está en la solución con la que se parte en cada iteración. En la primera iteración se parte de una solución aleatoria. Para las demás, se parte de una mutación de la mejor solución encontrada hasta el momento. Las mutaciones consisten en cambiar el valor situado en un tanto por ciento de las características, seleccionando aleatoriamente las características a cambiar.

	\begin{lstlisting}[mathescape=true]
ILS(data, categories) BEGIN	
   num_features $\leftarrow$ length ( data[0] )
   best_solution $\leftarrow$ {F,...,F: size = num_features}
   solution $\leftarrow$ random({True,False}, size = num_features)
   
   FOR i = 1,...,25
      IF solution is better than best_solution THEN
         best_solution $\leftarrow$ solution
         
      solution $\leftarrow$ mutateSolution(best_solution)      
   END
	
   return best_solution
END
	\end{lstlisting}
	
\section{Algoritmo de comparación}

Como algoritmo de comparación tenemos el algoritmo \textit{greedy} SFS. Partiendo de un vector con ninguna característica seleccionada, exploramos por el entorno y nos quedamos con el vecino que genera una mejor tasa de acierto. Repetimos este proceso hasta que ningún vecino aporta una mejora a la solución obtenida.

	\begin{lstlisting}[mathescape=true]
greedySFS(data, categories) BEGIN
   solution $\leftarrow$ {F,...,F: size = num_features}
   current_value $\leftarrow$ getValue(data, categories, solution)
   
   REPEAT
      neighbours $\leftarrow$ {flip(solution,i): i in characteristics}
   
      best_value $\leftarrow$ $\max_{\texttt{neighbours}}$ getValue(data, categories, $\cdot$)
      
      IF best_value > current_value THEN
         solution $\leftarrow$ $argmax_{\texttt{neighbours}}$ getValue(data, categories, $\cdot$)
   
   WHILE(best_value > current_value)
   
   RETURN solution
END
	\end{lstlisting}


\section{Procedimiento para desarrollar la práctica}

El código de la práctica está realizado en $\texttt{Python 3.5.1}$ y en \texttt{CUDA}. Como se ha comentado anteriormente, el código para el KNN está paralelizado usando el código de Alejandro García Montoro para usarlo con \textit{leave-one-out} y añadiéndole un método para usarlo como función de evaluación de la solución obtenida para el conjunto de test. Esto ha permitido reducir los tiempos considerablemente.\\
Los paquetes utilizados son:
\begin{enumerate}
	\item \texttt{scipy} para leer de una manera sencilla la base de datos.
	\item \texttt{numpy} para el manejo de vectores y matrices y tratar que sea algo más eficiente en lugar de las listas de \texttt{Python}.
	\item \texttt{ctype} para importar el generador de números aleatorios en \texttt{C} disponible en la página web de la asignatura. 
	\item \texttt{csv} para la lectura y escritura de ficheros \texttt{.csv} con los que manejar más cómodamente los datos.
	\item \texttt{pycuda} y \texttt{jinja2} para la paralelización en \texttt{CUDA}.
\end{enumerate}	

	La semilla con la que he realizado las ejecuciones es $3141592$, insertada tanto en el generador en $\texttt{C}$ como en el generador de números de $\texttt{numpy}$ y en el propio de \texttt{Python}. He usado los dos porque pretendía usar el primero, que es con el que se realizan las particiones, pero al llegar a los métodos que usan los generadores de números pseudoaleatorios en su funcionamiento me dí cuenta de que tendría que repetir el código de importación del módulo en \texttt{C} para cada método, por lo que opté por usar en los métodos el \texttt{random} de \texttt{numpy}. Mientras he ido realizando la práctica he ido creando funciones para permitir un más cómodo manejo del programa intentando que el código fuese entendible.
	
\subsection{Ejecución del programa}
La salida de cada ejecución (10 iteraciones de un algoritmo con una base de datos) se puede elegir entre mostrar por pantalla o redirigir a un archivo $\texttt{.csv}$ para manejarlo posteriormente, por ejemplo para incluir la tabla en \LaTeX.\\
Los parámetros que acepta el programa son:
\begin{itemize}
\item Base de datos: Será una letra $\texttt{W,L,A}$ que representa cada una de las bases de datos a utilizar. Este parámetro es el único obligatorio.
\item Algoritmo utilizado: Por defecto es el KNN. Para introducir uno distinto, se usa $\texttt{-a}$ seguido de una letra entre \texttt{K,S,B,G,I} que se corresponden con KNN, \textit{greedy} SFS, búsqueda multiarranque básica, \textit{GRASP} e \textit{iterated local search}, respectivamente.
\item Semilla. Para incluir una semilla, se añade $\texttt{-seed}$ seguido del número que usaremos como semilla. Por defecto es 3141592.
\item Salida por pantalla o a fichero. Se utiliza con el parámetro opcional \texttt{-write} para escribir en un fichero en una carpeta llamada \texttt{Resultados}. El nombre del fichero será la primera letra de la base de datos utilizada seguida por las iniciales del algoritmo. Incluye también la media, el mínimo, el máximo y la desviación típica para cada columna.
\item \texttt{-h} o \texttt{--help} para mostrar la ayuda y cómo se introducen los parámetros.
\end{itemize}

Por tanto, la ejecución del programa se hará de la siguiente manera:
\[ \texttt{python Practica1.py base\_de\_datos [-a algoritmo -seed semilla -write T/F ]} \]
Si por ejemplo queremos lanzar la base de datos de \texttt{WDBC} con GRASP, semilla 123456 y que los resultados se muestren por pantalla, escribimos
\[ \texttt{python Practica1.py W -a G -seed 123456}\]
Si optamos por la base de datos \texttt{Arrhythmia} con la búsqueda multiarranque básica y guardar el resultado en un fichero:
\[ \texttt{python Practica1.py A -a B -write True}\]
Para mostrar la introducción de parámetros:
\[ \texttt{python Practica1.py --help}\]

\section{Experimentos y análisis de resultados}

\subsection{Descripción de los casos}

Los casos del problema planteados son tres, cada uno de ellos asociado a una base de datos:

\begin{itemize}
\item WDBC: Base de datos con los atributos estimados a partir de una imagen de una aspiración de una masa en la mama. Tiene 569 ejemplos, 30 atributos y debemos clasificar cada individuo en dos valores.
\item Movement Libras: Base de datos con la representación de los movimientos de la mano en el lenguaje de signos LIBRAS. Tiene 360 ejemplos y consta de 91 atributos.
\item Arrhythmia: Contiene datos de pacientes durante la presencia y ausencia de arritmia cardíaca. Tiene 386 ejemplos y 254 atributos para categorizar en 5 clases. Reduje el número de características eliminando las columnas que tuvieran el mismo valor para todos los datos. Hice esto cuando tenía en la búsqueda local que se aceptasen soluciones iguales pero con menor número de características. Como he comentado, tardaba mucho así que pensé que en \texttt{Arrhythmia}, debido al gran número de datos y columnas, para cada búsqueda se gastaba demasiado tiempo en descartar estas 24 características cuando estuviesen seleccionadas. Sin embargo, debido a que buena parte del resto de columnas tienen valores parecidos para muchos de los elementos, el tiempo seguía siendo excesivo, con lo que descarté esa modificación de la búsqueda local.
\end{itemize}


\subsection{Resultados}
\subsubsection{KNN}
% Tablas Resumen KNN
\makeresume{\dataKNN}{\datawKNN}{\datalKNN}{\dataaKNN}

% Salida por pantalla
\maketable{\dataKNN}


En este caso el análisis es realmente el mismo que el de la primera práctica. Con el \texttt{KNN} se ve cómo es la BD en general y qué tasas de acierto se obtiene seleccionando todas las categorías. Las diferentes iteraciones y sus resultados no son más que para particiones distintas, pues la solución es la misma para todas las ejecuciones del algoritmo. Se ve que en \texttt{WDBC} y \texttt{Arrhythmia} se obtienen porcentajes similares para la clasificación dentro de la muestra y fuera, como cabría esperar, pues las particiones se hacen equilibradamente, y sin embargo en \texttt{Libras} el porcentaje de acierto fuera de la muestra es superior. Esto se puede deber a que hay un gran número de clases y no tantos representantes de esas clases como en las dos primeras bases de datos.

\subsubsection{SFS}

% Tablas Resumen SFS
\makeresume{\dataSFS}{\datawSFS}{\datalSFS}{\dataaSFS}

% Salida por pantalla
\maketable{\dataSFS}

Para el algoritmo \texttt{SFS} también se realizó el análisis en la práctica anterior, lo que nos permite comparar los tiempos entre haber usado el módulo de \texttt{scikit} y la paralelización realizada en esta práctica, siendo ahora en media más de 150 veces más rápido, para la base de datos de \texttt{Arrhythmia}, en otras BD y algoritmos la diferencia de tiempos puede seguir otra proporción.

\subsubsection{Búsqueda multiarranque básica}
% Tablas Resumen LS
\makeresume{\dataBMB}{\datawBMB}{\datalBMB}{\dataaBMB}

% Salida por pantalla
\maketable{\dataBMB}

En la búsqueda multiarranque básica vemos cómo llegamos a valores de la tasa de acierto (en especial dentro de la muestra) a las que no llegamos en la práctica anterior. Si observamos los resultados de la búsqueda local de la práctica anterior se comprueba que las mayores tasas de clasificación de entrenamiento de entre las diferentes particiones son similares a la media de las tasas obtenidas ahora. Y es que (aún siendo distintas particiones) si en la práctica anterior se realizaron 10 búsquedas locales por BD, ahora estamos haciendo 25 por partición. 


\subsubsection{GRASP}
% Tablas Resumen SA
\makeresume{\dataGRASP}{\datawGRASP}{\datalGRASP}{\dataaGRASP}

% Salida por pantalla
\maketable{\dataGRASP}

De los resultados de GRASP, teniendo en cuenta que parte de una solución \textit{greedy}, llama la atención la poca reducción de las características en \texttt{WDBC}, aunque si vemos que la tasa de acierto con el \texttt{KNN} es tan alta, ha podido ir tomando características de una en una de aquellas mejores que un umbral obteniendo siempre mejores resultados. En cambio para \texttt{Arrhythmia}, la reducción sí es mayor, fruto de que haya tantas características y no todas tengan realmente influencia. 

\subsubsection{ILS}
% Tablas Resumen TS
\makeresume{\dataILS}{\datawILS}{\datalILS}{\dataaILS}

% Salida por pantalla
\maketable{\dataILS}

En \texttt{ILS} vemos que se obtienen resultados similares a los de \texttt{BMB}, sólo un poco mejores. Esto se traduce en que pese a disponer un algoritmo simple para explorar vecindarios mayores a los considerados hasta ahora, el resultado está mejor dirigido que generar soluciones completamente aleatorias y aplicar búsqueda local. 

\subsubsection{Comparación}
 
\begin{adjustwidth}{-1cm}{}
\resizebox{\linewidth}{!}{
\pgfplotstabletypeset[
	every head row/.style={
		before row={%
				\hline
				& \multicolumn{4}{c|}{WDBC} & \multicolumn{4}{c|}{Movement Libras} & \multicolumn{4}{c|}{Arrhythmia}\\
				\cline{2-13}
		},
		after row=\cline{2-13}\hline,
		column type=c
	},
	columns={algorithm,inW,outW,redW,TW,inL,outL,redL,TL,inA,outA,redA,TA},
	every first column/.style={ column type/.add={|}{} },
	every last row/.style={after row=\hline},
	column type/.add={}{|},
	columns/algorithm/.style={column name = , string type},
	columns/inW/.style={sci,  precision=4,column name =\%Clas. in},
	columns/inL/.style={column name =\%Clas. in},	
	columns/inA/.style={column name =\%Clas. in},
	columns/outW/.style={column name =\%Clas. out},
	columns/outL/.style={column name =\%Clas. out},	
	columns/outA/.style={column name =\%Clas. out},
	columns/TW/.style={column name =T},
	columns/TA/.style={column name =T},
	columns/TL/.style={column name =T},
	columns/redW/.style={column name =\%red.},
	columns/redL/.style={column name =\%red.},	
	columns/redA/.style={column name =\%red.},
	string type
	]{\dataGlobal}
}
\end{adjustwidth}

\subsection{Análisis de los resultados}

\subsubsection{Tasa In}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
    ybar,
    x=2.1cm,
    xtick=data,% crucial line for the xticklabels directive
    ylabel=Tasa IN,
    ymin = 50,
    legend style={at={(0.5,-0.15)},anchor=north,legend columns=-1},
    xticklabels from table={\dataGlobal}{algorithm}
    %,colorbrewer cycle list=Set1
]
\addplot table [
    x=0,
    y=inW
]{\dataGlobal};

\addplot table [
    x=0,
    y=inL
]{\dataGlobal};

\addplot  table [
    x=0,
    y=inA
]{\dataGlobal};
\legend{WDBC,Libras,Arrhythmia}
\end{axis}
\end{tikzpicture}
\end{center}

En la tasa de acierto en la muestra de entrenamiento se aprecia una mejora en general de los algoritmos de esta práctica con respecto a los algoritmos de comparación (sólo en \texttt{Arrhythmia} \texttt{SFS} consigue una mejora con respecto a \texttt{BMB} e \texttt{ILS}). La mejor tasa de acierto se obtiene en \texttt{WDBC} y \texttt{Libras} con \texttt{ILS}, sin embargo en término medio el mejor es \texttt{GRASP}, pues para \texttt{Arrhytmia} obtiene unos resultados mejores sí con una cierta distancia y sin embargo, en las otras dos BD los resultados son sólo un poco peores. El mejor desempeño en esta base de datos se puede deber a que el número de características es mayor, con lo que resulta efectivo partir de la solución sin características seleccionadas y encontrar máximos locales, que serán soluciones donde sólo cuenten las características más relevantes.\\
\texttt{BMB} queda entonces como el algoritmo menos efectivo, pues, al encontrar óptimos locales en cada una de las veinticinco iteraciones desestima seguir buscando por el entorno de ese óptimo local y pega un salto. Sería interesante combinar \texttt{BMB} e \texttt{ILS}, realizando por ejemplo 5 búsquedas locales realizadas a partir de soluciones aleatorias, y por cada trayectoria, lanzar 5 búsquedas mutando la mejor solución encontrada en cada trayectoria.
 
\subsubsection{Tasa Out}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
    ybar,
    x=2.1cm,
    xtick=data,% crucial line for the xticklabels directive
    ylabel=Tasa OUT,
    ymin = 40,
    legend style={at={(0.5,-0.15)},anchor=north,legend columns=-1},
    xticklabels from table={\dataGlobal}{algorithm}
]
\addplot table [
    x=0,
    y=outW
]{\dataGlobal};

\addplot table [
    x=0,
    y=outL
]{\dataGlobal};

\addplot table [
    x=0,
    y=outA
]{\dataGlobal};
\legend{WDBC,Libras,Arrhythmia}
\end{axis}
\end{tikzpicture}
\end{center}

En la tasa de acierto en los datos de test resulta que considerar todas las características tiene un buen valor para \texttt{WDBC} y \texttt{Libras}. Sin embargo no consideraríamos esta opción en una situación real pues no estaríamos reduciendo el número de características, y como se observa en \texttt{Arrhythmia}, una base de datos con mayor número de clases y características, los resultados son bastante peores. De entre los demás algoritmos, es de nuevo \texttt{GRASP} el que obtiene los mejores resultados en término medio para las tres bases de datos, destacando especialmente en \texttt{Arrhythmia}.

\subsubsection{Tasa reducción}
\begin{center}
\begin{tikzpicture}
\begin{axis}[
    ybar,
    x=2.1cm,
    xtick=data,% crucial line for the xticklabels directive
    ylabel=Tasa red.,
    ymin = 0,
    legend style={at={(0.5,-0.15)},anchor=north,legend columns=-1},
    xticklabels from table={\dataGlobal}{algorithm}
]
\addplot table [
    x=0,
    y=redW
]{\dataGlobal};

\addplot table [
    x=0,
    y=redL
]{\dataGlobal};

\addplot table [
    x=0,
    y=redA
]{\dataGlobal};
\legend{WDBC,Libras,Arrhythmia}
\end{axis}
\end{tikzpicture}
\end{center}

Con la tasa de reducción se puede entender mejor la tasa de acierto. Se observa que en \texttt{Arrhythmia} se obtiene una mayor tasa de acierto en los datos de test a mayor tasa de reducción y en cambio para \texttt{WDBC} a menor tasa de reducción mayor acierto fuera de la muestra. Es relevante la tasa de reducción en \texttt{Arrhythmia} en \texttt{GRASP} debido a que los otros algoritmos empezaron con una solución aleatoria y se han quedado en óptimos locales que tenían en torno a un 50$\%$ de reducción y en cambio \texttt{GRASP}, empezando sin ninguna característica, ha parado antes obteniendo mejores resultados. Para las otras bases de datos este algoritmo también ha llegado a valores cercanos a los de los otros algoritmos, con lo que se deduce que es una buena estrategia de búsqueda. 

\subsubsection{Tiempos}

\pgfplotstabletranspose[input colnames to=DB,columns={TW,TL,TA}]\dataTimes{\dataGlobal}

\begin{center}
\pgfplotstabletypeset[string type]\dataTimes
\end{center}


\begin{center}
\begin{tikzpicture}
\begin{axis}[
    ybar,
    ylabel=Tiempo (s),
    ymin = 0,
    legend style={at={(1.2,0.7)},anchor=north},
    symbolic x coords={TW,TL,TA},
    xticklabels={WDBC,Libras,Arrhythmia},
    xtick=data% crucial line for the xticklabels directive
]
\addplot[sharp plot] 
	table [x=DB, y=0]	{\dataTimes};
\addplot[sharp plot,green] 
	table [x=DB, y=1]	{\dataTimes};
\addplot[sharp plot,blue] 
	table [x=DB, y=2]	{\dataTimes};
\addplot[sharp plot,red] 
	table [x=DB, y=3]	{\dataTimes};
\addplot[sharp plot,yellow] 
	table [x=DB, y=4]	{\dataTimes};
\legend{KNN,SFS,BMB,GRASP,ILS}
\end{axis}
\end{tikzpicture}
\end{center}

Observamos en la gráfica de tiempos que los tiempos son similares en los tres algoritmos de esta práctica para las dos primeras bases de datos, con \texttt{GRASP} un poco más lento. En cambio, \texttt{GRASP} es con diferencia es más rápido en \texttt{Arrhythmia} al quedarse con una buena solución muy pronto (alta tasa de reducción de características). También hay que destacar que, teniendo mejores resultados, \texttt{ILS} es más rápido que \texttt{BMB}, pues con cada mutación nos quedamos más cerca del óptimo que generando una solución aleatoria.

\section{Bibliografía}
\begin{itemize}
\item \href{http://scikit-learn.org/stable/modules/neighbors.html}{Módulo en \texttt{scikit} para KNN}
\item Para realizar las tablas en \LaTeX: \href{https://www.complang.tuwien.ac.at/doc/texlive-pictures-doc/latex/pgfplots/pgfplotstable.pdf}{Manual PGFPLOTSTABLE}
\end{itemize}
\end{document}
